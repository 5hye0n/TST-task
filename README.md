

### 📅 TST 성능 향상 프로젝트 로드맵 (4~6주 코스)

#### **1주차: 환경 구축 및 "채점기" 만들기 (Evaluation First)**
많은 학생들이 모델부터 만들지만, 성능을 높이려면 **'점수'를 매길 수 있는 도구**가 먼저 있어야 합니다.

*   **목표:** 내가 만든 모델이 잘하고 있는지 판단할 수 있는 평가 스크립트 완성.
*   **할 일:**
    1.  **데이터 수집:**
        *   비격식체(Informal) 데이터 10k 문장 (예: GYAFC 데이터셋, 트위터 크롤링, 드라마 대사).
        *   격식체(Formal) 데이터 10k 문장 (예: 위키피디아, 뉴스 기사, 논문).
    2.  **평가 지표 구현 (Python):**
        *   **스타일 분류기 (Style Classifier):** `bert-base-uncased`를 위 데이터로 파인튜닝하여 (Informal vs Formal) 분류 모델(정확도 90% 이상 목표) 만들기. [1, 2]
        *   **내용 보존 (Content Preservation):** `bert_score` 라이브러리 설치 및 점수 산출 테스트. [3]
        *   **유창성 (Fluency):** `GPT-2`를 불러와서 문장의 Perplexity(PPL) 계산하는 함수 작성. [2]
    3.  **베이스라인 측정:**
        *   ChatGPT(OpenAI API)나 Llama 3(Zero-shot)에게 "Change this to formal style:"이라고 시켜보고, 위 3가지 점수가 몇 점 나오는지 기록해 둡니다. (이 점수를 넘기는 것이 프로젝트의 목표가 됩니다.)

#### **2주차: "가짜 정답지" 만들기 (Data Engineering)**
병렬 데이터(쌍)가 없으므로, AI를 이용해 데이터를 짝지어 줍니다.

*   **목표:** 고품질 의사-병렬(Pseudo-Parallel) 학습 데이터셋(`train.csv`) 생성.
*   **할 일:**
    1.  **임베딩 추출:** `sentence-transformers` (모델명: `all-MiniLM-L6-v2` 추천)를 이용해 비격식체/격식체 문장들을 모두 벡터로 변환합니다.
    2.  **유사도 매칭 (Retrieval):** 비격식체 문장 하나당 가장 의미가 비슷한(Cosine Similarity가 높은) 격식체 문장을 찾아 짝을 짓습니다. [1, 4]
    3.  **데이터 정제 (Filtering):** **(중요)**
        *   유사도가 0.6 미만인 '엉터리 매칭'은 과감히 버립니다. [5]
        *   스타일 분류기를 돌려서, 짝지어진 격식체 문장이 실제로 '격식체'인지 한 번 더 검증합니다.
        *   이렇게 정제된 데이터를 `train.csv`로 저장합니다.

#### **3주차: 모델 훈련하기 (PEFT Fine-tuning)**
가장 핵심적인 단계입니다. 가벼운 모델을 LoRA로 학습시킵니다.

*   **목표:** 나만의 TST 전용 모델 확보.
*   **할 일:**
    1.  **모델 선정:**
        *   Google Colab 무료 버전을 쓴다면: `google/flan-t5-base` (seq2seq 모델이라 TST에 아주 강함).
        *   게이밍 GPU(VRAM 12GB 이상)가 있다면: `Llama-3-8B` 또는 `Gemma-2-9B` (4bit 양자화 필수).
    2.  **LoRA 학습 (PEFT):**
        *   Hugging Face `peft` 라이브러리 사용.
        *   `train.csv`를 로드하여 학습 진행 (Epoch은 3~5회 정도면 충분). [6]
    3.  **체크포인트 저장:** 학습된 LoRA 어댑터 파일(`adapter_model.bin`)을 저장합니다.

#### **4주차: 추론, 평가 및 "후처리" (Refinement)**
결과물을 뽑아보고, 부족한 유창성을 보완합니다.

*   **목표:** 최종 결과물 생성 및 베이스라인 대비 성능 향상 확인.
*   **할 일:**
    1.  **추론 (Inference):** 테스트 데이터(학습에 안 쓴 문장)를 모델에 넣어 변환 결과를 뽑습니다.
    2.  **1차 평가:** 1주차에 만든 채점기로 점수를 매겨봅니다. (아마 베이스라인보다 스타일 점수는 높지만, 문법이 약간 깨질 수 있습니다.)
    3.  **후처리 (Post-processing) 적용:** [1, 7]
        *   `Grammar Correction` 모델(Hugging Face에 `vennify/t5-base-grammar-correction` 같은 모델이 많습니다)을 가져옵니다.
        *   내 모델이 뱉은 결과물을 이 문법 교정 모델에 한 번 더 통과시킵니다.
    4.  **최종 평가:** 후처리까지 거친 최종 문장의 점수를 측정합니다.

#### **5주차 (선택): 보고서 작성 및 시각화**
*   **할 일:**
    *   **비교표 작성:** [Zero-shot(베이스라인)] vs vs [내 모델 + 후처리] 3가지의 점수 비교 표를 만듭니다.
    *   **성공/실패 사례 분석:** 점수가 잘 나온 문장과, 내용이 왜곡된 실패 문장을 뽑아서 분석합니다. (이 부분이 프로젝트 점수에 큰 영향을 줍니다.)

---

### 💡 학부생을 위한 팁 (성능을 쥐어짜는 방법)

1.  **데이터 필터링이 전부다:** 모델 아키텍처를 고민하는 것보다, 2주차에서 **'유사도가 높으면서 스타일이 확실히 다른'** 데이터 쌍만 남기는 게 성능 향상 폭이 훨씬 큽니다. (Garbage In, Garbage Out) [5]
2.  **프롬프트 튜닝:** T5나 Llama 학습 시 입력 데이터 앞에 붙이는 프롬프트(예: `"Transfer this text to formal style: "`)를 조금씩 바꿔보세요. 의외로 큰 차이가 날 수 있습니다. [3]
3.  **작은 모델이 더 나을 수도:** TST는 문장 구조를 바꾸는 작업이라, 너무 거대한 LLM(70B)보다 T5-base 같은 seq2seq 모델이 더 빠르고 안정적으로 학습될 때가 많습니다.
